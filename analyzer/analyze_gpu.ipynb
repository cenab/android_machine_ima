{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u95yGDnrEjo",
        "outputId": "2afef64d-4ea5-4a82-8cc2-5156aedc3fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (18.160.213.72)] [Connected to r2u.stat.i\r                                                                                                    \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)]\r                                                                                                    \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcon\r                                                                                                    \rHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: thundersvm in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->thundersvm) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->thundersvm) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# Install RAPIDS cuML and ThunderSVM\n",
        "# This script installs RAPIDS cuML for CUDA 11.2 and ThunderSVM\n",
        "# Ensure that the runtime is set to use GPU (see section 4)\n",
        "\n",
        "# Install dependencies\n",
        "!apt-get update\n",
        "!pip install pandas\n",
        "\n",
        "# Install ThunderSVM via pip\n",
        "!pip install thundersvm\n",
        "\n",
        "# Verify installations\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM60XxPYeZc1",
        "outputId": "79895a9b-461a-44d0-83a3-2abef3c5e882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 11 09:59:25 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZVkmryneOpJ",
        "outputId": "64a4f7d9-0f43-4ab8-fc4b-0000bd3a6540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n",
            "Installing RAPIDS remaining 24.4.* libraries\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.1)\n",
            "Requirement already satisfied: cuml-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: cugraph-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: cuspatial-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: cuproj-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: cuxfilter-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.1)\n",
            "Requirement already satisfied: cucim-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: pylibraft-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: raft-dask-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: nx-cugraph-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.9)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (5.5.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (12.2.1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (12.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (2024.6.1)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.60.0)\n",
            "Requirement already satisfied: numpy<2.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (1.26.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (24.1)\n",
            "Requirement already satisfied: pandas<2.2.2dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (2.2.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (3.20.3)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.3.0)\n",
            "Requirement already satisfied: pyarrow<15.0.0a0,>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (14.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (13.9.2)\n",
            "Requirement already satisfied: rmm-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (24.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (4.12.2)\n",
            "Requirement already satisfied: dask-cuda==24.4.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (24.4.0)\n",
            "Requirement already satisfied: dask-cudf-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (24.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (1.4.2)\n",
            "Requirement already satisfied: rapids-dask-dependency==24.4.* in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (24.4.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (1.13.1)\n",
            "Requirement already satisfied: treelite==4.1.2 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (4.1.2)\n",
            "Requirement already satisfied: pylibcugraph-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (from cugraph-cu12==24.4.*) (24.4.0)\n",
            "Requirement already satisfied: ucx-py-cu12==0.37.* in /usr/local/lib/python3.10/dist-packages (from cugraph-cu12==24.4.*) (0.37.0)\n",
            "Requirement already satisfied: geopandas>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from cuspatial-cu12==24.4.*) (1.0.1)\n",
            "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (3.4.3)\n",
            "Requirement already satisfied: datashader>=0.15 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (0.16.3)\n",
            "Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (1.19.1)\n",
            "Requirement already satisfied: jupyter-server-proxy in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (4.4.0)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (1.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (8.1.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (0.4)\n",
            "Requirement already satisfied: scikit-image<0.23.0a0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (0.22.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12==24.4.*) (3.3)\n",
            "Requirement already satisfied: pynvml<11.5,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==24.4.*->cuml-cu12==24.4.*) (11.4.1)\n",
            "Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
            "Requirement already satisfied: dask==2024.1.1 in /usr/local/lib/python3.10/dist-packages (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2024.1.1)\n",
            "Requirement already satisfied: distributed==2024.1.1 in /usr/local/lib/python3.10/dist-packages (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2024.1.1)\n",
            "Requirement already satisfied: dask-expr==0.4.0 in /usr/local/lib/python3.10/dist-packages (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (0.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.2.1)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (8.4.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.1.4)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (1.3.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (10.4.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (2024.9.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12==24.4.*) (3.0.11)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==24.4.*) (0.8.2)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (3.1.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.1.1)\n",
            "Requirement already satisfied: pyct in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (0.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.32.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2024.9.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (0.10.0)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (3.7.0)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2.0.6)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.16.0->cuxfilter-cu12==24.4.*) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12==24.4.*) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.0.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (4.66.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (6.1.0)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2024.9.20)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: jupyter-server>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.24.0)\n",
            "Requirement already satisfied: simpervisor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.0.0)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12==24.4.*) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.1.5)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (23.1.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.2)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.10.4)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.21.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (24.0.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.18.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (0.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyogrio>=0.7.2->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->cuxfilter-cu12==24.4.*) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (1.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader>=0.15->cuxfilter-cu12==24.4.*) (3.3.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.2.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.20.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.12.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.20.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.22)\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "        \n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "        \n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: <ipython-input-11-1cffd0fbceb9> in <cell line: 12>()\n",
        "#      10 import cudf\n",
        "#      11 from cuml.ensemble import RandomForestClassifier as cuRF\n",
        "# ---> 12 from cuml.ensemble import GradientBoostingClassifier as cuGB\n",
        "#      13 from cuml.linear_model import LogisticRegression as cuLR\n",
        "#      14 from cuml.svm import SVC as cuSVC\n",
        "# ImportError: cannot import name 'GradientBoostingClassifier' from 'cuml.ensemble' (/u\n",
        "\n",
        "# Install RAPIDS cuML and ThunderSVM\n",
        "# This script installs RAPIDS cuML for CUDA 11.2 and ThunderSVM\n",
        "# Ensure that the runtime is set to use GPU (see section 4)\n",
        "\n",
        "# Install dependencies\n",
        "!apt-get update\n",
        "!pip install pandas\n",
        "\n",
        "# Install ThunderSVM via pip\n",
        "!pip install thundersvm\n",
        "\n",
        "import cuml\n",
        "print(cuml.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3LRq5mjdeBJ",
        "outputId": "403bee67-d325-4214-ab4a-d21bfd2dbabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: thundersvm in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->thundersvm) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->thundersvm) (3.5.0)\n",
            "24.04.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4xZuBmJmqCz",
        "outputId": "4993d0fb-b724-49e1-ddac-5f0b2a248732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "print(\"cuML version:\", cuml.__version__)\n",
        "\n",
        "import xgboost as xgb\n",
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# Verify GPU availability for XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "model = XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
        "print(\"XGBClassifier initialized with GPU support.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjxRqP3Hnw99",
        "outputId": "62bdce9f-1d8f-47ab-ce3b-00feeee49bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuML version: 24.04.00\n",
            "XGBoost version: 2.1.1\n",
            "XGBClassifier initialized with GPU support.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Update package lists quietly\n",
        "apt-get update -qq\n",
        "\n",
        "# Install the locales package\n",
        "apt-get install -y locales\n",
        "\n",
        "# Generate the en_US.UTF-8 locale\n",
        "locale-gen en_US.UTF-8\n",
        "\n",
        "# Update the locale settings\n",
        "update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8\n",
        "\n",
        "# Verify the locale settings\n",
        "locale -a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykwh3IE31Hnh",
        "outputId": "e29c5745-6fa9-48ce-f2ef-0a2a92c42ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "locales is already the newest version (2.35-0ubuntu3.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Generating locales (this might take a while)...\n",
            "  en_US.UTF-8... done\n",
            "Generation complete.\n",
            "C\n",
            "C.utf8\n",
            "en_US.utf8\n",
            "POSIX\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import locale\n",
        "\n",
        "# Set locale environment variables in Python\n",
        "os.environ['LANG'] = 'en_us.utf-8'\n",
        "os.environ['LC_ALL'] = 'en_us.utf-8'\n",
        "\n",
        "try:\n",
        "    # Attempt to set the locale\n",
        "    locale.setlocale(locale.LC_ALL, 'en_us.utf-8')\n",
        "    print(\"Locale successfully set to:\", locale.getlocale())\n",
        "    print(\"Locale encoding:\", locale.getpreferredencoding())\n",
        "except locale.Error as e:\n",
        "    print(\"Error setting locale:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6ufyt9hlU4K",
        "outputId": "3d75c287-67b5-4082-e603-a0b7f46ea569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error setting locale: unsupported locale setting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "hAGCHFGF4E_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "!pip install scikit-learn==1.2.2\n",
        "!pip install eli5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2mo8C_J1Qv4",
        "outputId": "59def16a-1c62-42b7-a115-6dd9feebb366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "Requirement already satisfied: eli5 in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: attrs>17.1.0 in /usr/local/lib/python3.10/dist-packages (from eli5) (24.2.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from eli5) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from eli5) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from eli5) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from eli5) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from eli5) (1.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from eli5) (0.20.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from eli5) (0.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.0->eli5) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->eli5) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->eli5) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H164kpGRfkaU",
        "outputId": "95e94328-e409-434f-e27e-931e24580b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzUUiRO1qYN6",
        "outputId": "5cd6e336-de09-420f-be27-41d38abbb5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.3)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "!pip install imblearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtDVnfeGAAzP",
        "outputId": "3ffb7d59-7067-402a-8f66-97c15cf0e863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_us.utf-8)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.10/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"\n",
        "Usage:\n",
        "    python analyze.py <experiment_name> <function>\n",
        "\n",
        "Functions:\n",
        "    - process: Processes data and trains models with feature selection, cross-validation, and hyperparameter tuning.\n",
        "    - count: Builds a count table of flows per application and per device.\n",
        "    - eda: Performs Exploratory Data Analysis on the dataset.\n",
        "    - train_model: Trains a selected model and saves it for future use.\n",
        "\n",
        "Examples:\n",
        "    python analyze.py experiment1 process\n",
        "    python analyze.py experiment1 count\n",
        "    python analyze.py experiment1 eda\n",
        "    python analyze.py experiment1 train_model\n",
        "\"\"\"\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import joblib\n",
        "\n",
        "# Import classifiers\n",
        "from cuml.ensemble import RandomForestClassifier as cuRF\n",
        "from cuml.linear_model import LogisticRegression as cuLR\n",
        "from cuml.svm import SVC as cuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier as skDecisionTree\n",
        "from sklearn.ensemble import RandomForestClassifier as skRF\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Import feature selection and resampling tools\n",
        "from sklearn.feature_selection import mutual_info_classif as sk_mutual_info_classif\n",
        "from sklearn.feature_selection import SelectKBest as skSelectKBest\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Import model evaluation tools\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    f1_score as sk_f1_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    make_scorer,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "from cuml.metrics import confusion_matrix as cu_confusion_matrix\n",
        "\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "LABEL_COLUMNS = ['application', 'device']\n",
        "\n",
        "# Define applications and their full names\n",
        "apps = [\"discord\", \"messenger\", \"rocket\", \"signal\", \"skype\", \"slack\", \"telegram\", \"teams\"]\n",
        "apps_fullname = [\"Discord\", \"Messenger\", \"Rocket\", \"Signal\", \"Skype\", \"Slack\", \"Telegram\", \"Teams\"]\n",
        "\n",
        "# Define devices\n",
        "devices = [\"flow1\", \"flow2\", \"flow3\"]\n",
        "devices_fullname = [\"Device1\", \"Device2\", \"Device3\"]\n",
        "\n",
        "# Mapping for label encoding based on type\n",
        "app_to_num = {app.lower(): idx for idx, app in enumerate(apps_fullname)}\n",
        "device_to_num = {device.lower(): idx for idx, device in enumerate(devices_fullname)}\n",
        "\n",
        "# Define classifiers dictionary with initial classifiers\n",
        "cdic = {\n",
        "    # \"DecisionTree\": skDecisionTree(class_weight='balanced'),\n",
        "    # \"RandomForest\": cuRF(n_estimators=100),\n",
        "    # \"GradientBoosting\": XGBClassifier(\n",
        "    #     tree_method='gpu_hist',\n",
        "    #     gpu_id=0,\n",
        "    #     n_estimators=100,\n",
        "    #     use_label_encoder=False,  # Remove or ensure it's set to False\n",
        "    #     eval_metric='mlogloss'\n",
        "    # ),\n",
        "    # \"LogisticRegression\": cuLR(max_iter=1000),\n",
        "    # \"NaiveBayes\": GaussianNB(),  # Does not support class_weight\n",
        "    \"SVM\": cuSVC(kernel='rbf', C=1.0, probability=True),  # Supports class_weight\n",
        "}\n",
        "\n",
        "cdic_names = list(cdic.keys())\n",
        "\n",
        "# Define paths relative to the current script location\n",
        "base_dir = Path('/content/drive/MyDrive/Colab Notebooks/flows')  # Adjust as needed\n",
        "flow_dirs = [base_dir / f\"flow{i}\" for i in range(1, 4)]  # flow1, flow2, flow3 representing different devices\n",
        "plots_root = base_dir / \"plots\"      # Directory to save plots\n",
        "csvs = base_dir / \"csvs\"             # Directory to save CSVs\n",
        "models_dir = base_dir / \"models\"     # Directory to save trained models\n",
        "\n",
        "# Ensure directories exist\n",
        "plots_root.mkdir(parents=True, exist_ok=True)\n",
        "csvs.mkdir(parents=True, exist_ok=True)\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Custom Scoring Function\n",
        "# -------------------------------\n",
        "\n",
        "def multioutput_f1_macro(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the mean F1 macro score for multi-output classification.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true (np.ndarray): True labels.\n",
        "    - y_pred (np.ndarray): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "    - float: Mean F1 macro score across all outputs.\n",
        "    \"\"\"\n",
        "    f1_scores = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        f1 = sk_f1_score(y_true[:, i], y_pred[:, i], average='macro', zero_division=0)\n",
        "        f1_scores.append(f1)\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "multioutput_scorer = make_scorer(multioutput_f1_macro)\n",
        "\n",
        "# -------------------------------\n",
        "# Logging Configuration\n",
        "# -------------------------------\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"analyze.log\"),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -------------------------------\n",
        "# Utility Functions\n",
        "# -------------------------------\n",
        "\n",
        "def export_df(df: pd.DataFrame, filepath: Path):\n",
        "    \"\"\"\n",
        "    Exports a DataFrame to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame to export.\n",
        "    - filepath (Path): The file path where the CSV will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df.to_csv(filepath, index=False)\n",
        "        logger.info(f\"DataFrame exported to {filepath}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error exporting DataFrame to {filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "def export_cm(cm: np.ndarray, labels: list, filepath: Path, title: str):\n",
        "    \"\"\"\n",
        "    Exports a confusion matrix as a heatmap image.\n",
        "\n",
        "    Parameters:\n",
        "    - cm (np.ndarray): The confusion matrix.\n",
        "    - labels (list): The labels for the axes.\n",
        "    - filepath (Path): The file path where the heatmap image will be saved.\n",
        "    - title (str): Title for the confusion matrix plot.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filepath)\n",
        "        plt.close()\n",
        "        logger.info(f\"Confusion matrix heatmap saved to {filepath}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error exporting confusion matrix to {filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "def export_feature_importance(importances: pd.DataFrame, filepath: Path, title: str):\n",
        "    \"\"\"\n",
        "    Exports feature importances as a bar plot.\n",
        "\n",
        "    Parameters:\n",
        "    - importances (pd.DataFrame): DataFrame containing feature importances.\n",
        "    - filepath (Path): The file path where the plot will be saved.\n",
        "    - title (str): Title for the feature importance plot.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=importances.sort_values(by='Importance', ascending=False))\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(filepath)\n",
        "        plt.close()\n",
        "        logger.info(f\"Feature importance plot saved to {filepath}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error exporting feature importance to {filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "def read_flow_data(filepath: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads flow data from a specified file with robust encoding handling.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath (Path): Path to the flow data file.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame containing the flow data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            filepath,\n",
        "            delimiter=r'\\s+',\n",
        "            index_col=False,\n",
        "            encoding='utf-8',\n",
        "            on_bad_lines='skip'  # Replaces deprecated 'error_bad_lines' and 'warn_bad_lines'\n",
        "        )\n",
        "        logger.info(f\"Successfully read {filepath} with 'utf-8' encoding.\")\n",
        "    except UnicodeDecodeError as e:\n",
        "        logger.error(f\"Unicode decoding error for file {filepath}: {e}\")\n",
        "        # Attempt reading with 'latin1' as a fallback\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                filepath,\n",
        "                delimiter=r'\\s+',\n",
        "                index_col=False,\n",
        "                encoding='latin1',\n",
        "                on_bad_lines='skip'\n",
        "            )\n",
        "            logger.info(f\"Successfully read {filepath} with 'latin1' encoding.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to read {filepath} with fallback encoding: {e}\")\n",
        "            raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error while reading {filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "    if df.empty:\n",
        "        logger.warning(f\"No data found in {filepath}. Skipping.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------\n",
        "# Comprehensive choose_features Function\n",
        "# -------------------------------\n",
        "\n",
        "def feature_selection(comb: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Selects top features based on Mutual Information.\n",
        "\n",
        "    Parameters:\n",
        "    - comb (pd.DataFrame): Combined DataFrame with features and labels.\n",
        "    - name (str): Experiment name.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with selected features and encoded labels.\n",
        "    \"\"\"\n",
        "    selector = skSelectKBest(score_func=sk_mutual_info_classif, k=20)\n",
        "    X_selected = selector.fit_transform(comb.drop(LABEL_COLUMNS, axis=1), comb['application'])\n",
        "    selected_features = selector.get_support(indices=True)\n",
        "    feature_names = comb.drop(LABEL_COLUMNS, axis=1).columns[selected_features]\n",
        "    comb_selected = pd.DataFrame(X_selected, columns=feature_names)\n",
        "    comb_selected[LABEL_COLUMNS] = comb[LABEL_COLUMNS].reset_index(drop=True)\n",
        "    logger.info(f\"Selected features after Mutual Information: {len(feature_names)}\")\n",
        "    return comb_selected\n",
        "\n",
        "def anova_feature_selection(comb: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Selects top features based on ANOVA F-test.\n",
        "\n",
        "    Parameters:\n",
        "    - comb (pd.DataFrame): Combined DataFrame with features and labels.\n",
        "    - name (str): Experiment name.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with selected features and encoded labels.\n",
        "    \"\"\"\n",
        "    selector = skSelectKBest(score_func=f_classif, k=20)\n",
        "    X_selected = selector.fit_transform(comb.drop(LABEL_COLUMNS, axis=1), comb['application'])\n",
        "    selected_features = selector.get_support(indices=True)\n",
        "    feature_names = comb.drop(LABEL_COLUMNS, axis=1).columns[selected_features]\n",
        "    comb_selected = pd.DataFrame(X_selected, columns=feature_names)\n",
        "    comb_selected[LABEL_COLUMNS] = comb[LABEL_COLUMNS].reset_index(drop=True)\n",
        "    logger.info(f\"Selected {len(feature_names)} features based on ANOVA F-test.\")\n",
        "    return comb_selected\n",
        "\n",
        "def choose_features(comb: pd.DataFrame, features: str, name: str, typee: str = \"intra\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Chooses features based on the specified feature type.\n",
        "\n",
        "    Parameters:\n",
        "    - comb (pd.DataFrame): Combined DataFrame with features and labels.\n",
        "    - features (str): Feature selection type ('all', 'categorical', 'statistical', 'custom_categorical',\n",
        "                      'custom_statistical', 'mutual_info', 'anova_f_test', 'polynomial').\n",
        "    - name (str): Experiment name used for exporting results.\n",
        "    - typee (str): Type of label encoding ('intra' or 'inter').\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with selected features and encoded labels.\n",
        "    \"\"\"\n",
        "    # Encode labels\n",
        "    if typee == \"inter\":\n",
        "        comb[\"application\"] = comb[\"application\"].str.lower().map(app_to_num)\n",
        "    else:\n",
        "        # Convert application names to lowercase before mapping\n",
        "        comb[\"application\"] = comb[\"application\"].str.lower().map(app_to_num)\n",
        "\n",
        "    # Convert device names to lowercase before mapping\n",
        "    comb[\"device\"] = comb[\"device\"].str.lower().map(device_to_num)\n",
        "\n",
        "    # Check for unmapped labels and handle them gracefully\n",
        "    if comb[\"application\"].isnull().any() or comb[\"device\"].isnull().any():\n",
        "        unmapped_apps = comb[comb[\"application\"].isnull()][\"application\"].unique()\n",
        "        unmapped_devices = comb[comb[\"device\"].isnull()][\"device\"].unique()\n",
        "        logger.warning(f\"Found unmapped labels: Applications - {unmapped_apps}, Devices - {unmapped_devices}\")\n",
        "        # Instead of exiting, you might want to drop or handle these rows differently\n",
        "        comb = comb.dropna(subset=[\"application\", \"device\"])  # Drop rows with unmapped labels\n",
        "\n",
        "    # Fill missing values\n",
        "    comb = comb.fillna(0)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    comb = comb.drop([\"timeFirst\", \"timeLast\", \"flowInd\"], axis=1, errors='ignore')\n",
        "    to_drop = []  # Add columns to drop if necessary\n",
        "    comb = comb.drop(columns=[col for col in to_drop if col in comb.columns], errors='ignore')\n",
        "\n",
        "    # Identify categorical columns (excluding label columns)\n",
        "    categorical_cols = comb.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    categorical_cols = [col for col in categorical_cols if col not in LABEL_COLUMNS]\n",
        "\n",
        "    if categorical_cols:\n",
        "        logger.info(f\"One-hot encoding categorical features: {categorical_cols}\")\n",
        "        comb = pd.get_dummies(comb, columns=categorical_cols, drop_first=True)\n",
        "        logger.info(\"Categorical features successfully one-hot encoded.\")\n",
        "    else:\n",
        "        logger.info(\"No categorical features to encode.\")\n",
        "\n",
        "    # Debugging: Log data types after one-hot encoding\n",
        "    logger.info(\"Data types after one-hot encoding:\")\n",
        "    logger.info(comb.dtypes)\n",
        "\n",
        "    # Verify all features are numeric, excluding label columns\n",
        "    remaining_non_numeric = comb.drop(columns=LABEL_COLUMNS, errors='ignore').select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Debugging: Log remaining non-numeric columns after exclusion\n",
        "    logger.info(f\"Remaining non-numeric columns after exclusion: {remaining_non_numeric}\")\n",
        "\n",
        "    if remaining_non_numeric:\n",
        "        logger.error(f\"Some columns remain non-numeric: {remaining_non_numeric}\")\n",
        "        raise ValueError(\"Not all features have been converted to numeric.\")\n",
        "    else:\n",
        "        logger.info(\"All features are now numeric.\")\n",
        "\n",
        "    # Select features based on the 'features' parameter\n",
        "    if features == \"all\":\n",
        "        logger.info(\"Using all features.\")\n",
        "        comb_selected = comb.copy()\n",
        "\n",
        "    elif features == \"categorical\":\n",
        "        logger.info(\"Selecting categorical features: application, device, and original categorical columns.\")\n",
        "        comb_selected = comb[LABEL_COLUMNS + categorical_cols].copy()\n",
        "\n",
        "    elif features == \"statistical\":\n",
        "        logger.info(\"Selecting statistical features by dropping categorical columns.\")\n",
        "        comb_selected = comb.drop(columns=categorical_cols, errors='ignore').copy()\n",
        "\n",
        "    elif features == \"custom_categorical\":\n",
        "        logger.info(\"Selecting custom categorical features.\")\n",
        "        custom_cat_cols = [\"dstIPOrg\", \"srcIPOrg\", \"%dir\", \"dstPortClass\"]\n",
        "        available_cols = [col for col in custom_cat_cols if col in comb.columns]\n",
        "        comb_selected = comb[LABEL_COLUMNS + available_cols].copy()\n",
        "        logger.info(f\"Selected custom categorical columns: {available_cols}\")\n",
        "\n",
        "    elif features == \"custom_statistical\":\n",
        "        logger.info(\"Selecting custom statistical features.\")\n",
        "        custom_stat_cols = [\n",
        "            \"numPktsSnt\", \"numPktsRcvd\", \"numBytesSnt\", \"numBytesRcvd\",\n",
        "            \"minPktSz\", \"maxPktSz\", \"avePktSize\", \"stdPktSize\",\n",
        "            \"minIAT\", \"maxIAT\", \"aveIAT\", \"stdIAT\", \"bytps\"\n",
        "        ]\n",
        "        available_cols = [col for col in custom_stat_cols if col in comb.columns]\n",
        "        comb_selected = comb[LABEL_COLUMNS + available_cols].copy()\n",
        "        logger.info(f\"Selected custom statistical columns: {available_cols}\")\n",
        "\n",
        "    elif features == \"mutual_info\":\n",
        "        logger.info(\"Selecting features based on Mutual Information.\")\n",
        "        comb_selected = feature_selection(comb, name)\n",
        "\n",
        "    elif features == \"anova_f_test\":\n",
        "        logger.info(\"Selecting features based on ANOVA F-test.\")\n",
        "        comb_selected = anova_feature_selection(comb, name)\n",
        "\n",
        "    elif features == \"polynomial\":\n",
        "        logger.info(\"Adding Polynomial Features.\")\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "        feature_cols = comb.drop(LABEL_COLUMNS, axis=1).columns\n",
        "        X_poly = poly.fit_transform(comb.drop(LABEL_COLUMNS, axis=1))\n",
        "        poly_feature_names = poly.get_feature_names_out(feature_cols)\n",
        "        comb_poly = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
        "        comb_selected = pd.concat([comb_poly, comb[LABEL_COLUMNS].reset_index(drop=True)], axis=1)\n",
        "        logger.info(f\"Polynomial features added. Total features: {comb_selected.shape[1] - len(LABEL_COLUMNS)}\")\n",
        "\n",
        "    else:\n",
        "        logger.warning(f\"Feature selection type '{features}' not recognized. Using all features.\")\n",
        "        comb_selected = comb.copy()\n",
        "\n",
        "    return comb_selected\n",
        "\n",
        "def handle_imbalance(X, y, strategy=\"class_weight\"):\n",
        "    \"\"\"\n",
        "    Handles class imbalance using the specified strategy.\n",
        "\n",
        "    Parameters:\n",
        "    - X (np.ndarray or pd.DataFrame): Feature matrix.\n",
        "    - y (np.ndarray): Target matrix.\n",
        "    - strategy (str): Imbalance handling strategy.\n",
        "\n",
        "    Returns:\n",
        "    - X_resampled, y_resampled: Resampled feature and target matrices.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Handling class imbalance using strategy: {strategy}\")\n",
        "\n",
        "    # Check if y is multi-output\n",
        "    if y.ndim > 1 and y.shape[1] > 1:\n",
        "        if strategy not in [\"class_weight\"]:\n",
        "            logger.warning(\n",
        "                f\"Imbalance strategy '{strategy}' is not supported for multi-output targets. \"\n",
        "                f\"Setting imbalance_strategy to 'class_weight'.\"\n",
        "            )\n",
        "            strategy = \"class_weight\"\n",
        "        else:\n",
        "            logger.info(\"Multi-output classification detected.\")\n",
        "\n",
        "    if strategy == \"smoteenn\":\n",
        "        from imblearn.combine import SMOTEENN\n",
        "        smote_enn = SMOTEENN(random_state=42)\n",
        "        X_res, y_res = smote_enn.fit_resample(X, y)\n",
        "    elif strategy == \"smote\":\n",
        "        from imblearn.over_sampling import SMOTE\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_res, y_res = smote.fit_resample(X, y)\n",
        "    elif strategy == \"undersample\":\n",
        "        from imblearn.under_sampling import RandomUnderSampler\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        X_res, y_res = rus.fit_resample(X, y)\n",
        "    elif strategy == \"class_weight\":\n",
        "        # No resampling needed; handled via class_weight in classifiers\n",
        "        X_res, y_res = X, y\n",
        "        logger.info(\"Using 'class_weight' strategy; no resampling performed.\")\n",
        "    else:\n",
        "        logger.error(f\"Unknown imbalance strategy: {strategy}\")\n",
        "        raise ValueError(f\"Unknown imbalance strategy: {strategy}\")\n",
        "    return X_res, y_res\n",
        "\n",
        "def perform_eda(comb: pd.DataFrame, name: str):\n",
        "    \"\"\"\n",
        "    Performs Exploratory Data Analysis (EDA) and saves the results.\n",
        "\n",
        "    Parameters:\n",
        "    - comb (pd.DataFrame): Combined DataFrame.\n",
        "    - name (str): Experiment name.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Performing EDA for experiment '{name}'.\")\n",
        "    try:\n",
        "        # Example EDA: distribution of applications\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(x='application', data=comb)\n",
        "        plt.title(f\"Application Distribution - {name}\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(plots_root / f\"{name}_application_distribution.png\")\n",
        "        plt.close()\n",
        "        logger.info(f\"Application distribution plot saved to {plots_root / f'{name}_application_distribution.png'}.\")\n",
        "\n",
        "        # Example EDA: correlation heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(comb.drop(['application', 'device'], axis=1).corr(), annot=False, cmap='coolwarm')\n",
        "        plt.title(f\"Feature Correlation Heatmap - {name}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(plots_root / f\"{name}_correlation_heatmap.png\")\n",
        "        plt.close()\n",
        "        logger.info(f\"Correlation heatmap saved to {plots_root / f'{name}_correlation_heatmap.png'}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during EDA: {e}\")\n",
        "        raise\n",
        "\n",
        "def build_count_table(comb: pd.DataFrame, name: str):\n",
        "    \"\"\"\n",
        "    Builds and exports count tables of flows per application and per device.\n",
        "\n",
        "    Parameters:\n",
        "    - comb (pd.DataFrame): Combined DataFrame.\n",
        "    - name (str): Experiment name.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Building count tables for experiment '{name}'.\")\n",
        "\n",
        "    try:\n",
        "        count_app = comb['application'].value_counts().rename_axis('application').reset_index(name='counts')\n",
        "        export_df(count_app, csvs / f\"{name}_application_counts.csv\")\n",
        "        logger.info(f\"Application counts exported to {csvs / f'{name}_application_counts.csv'}.\")\n",
        "\n",
        "        count_dev = comb['device'].value_counts().rename_axis('device').reset_index(name='counts')\n",
        "        export_df(count_dev, csvs / f\"{name}_device_counts.csv\")\n",
        "        logger.info(f\"Device counts exported to {csvs / f'{name}_device_counts.csv'}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error building count tables: {e}\")\n",
        "        raise\n",
        "\n",
        "# -------------------------------\n",
        "# Feature Importance Analysis\n",
        "# -------------------------------\n",
        "\n",
        "def analyze_feature_importance(clf, X, y, feature_names, name, clf_name):\n",
        "    \"\"\"\n",
        "    Analyzes and exports feature importances from the trained classifier using coefficients or permutation importance.\n",
        "\n",
        "    Parameters:\n",
        "    - clf: Trained classifier.\n",
        "    - X (pd.DataFrame): Feature dataset.\n",
        "    - y (np.ndarray): Target labels.\n",
        "    - feature_names (list): List of feature names.\n",
        "    - name (str): Experiment name.\n",
        "    - clf_name (str): Classifier name.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Analyzing feature importances for classifier '{clf_name}'.\")\n",
        "\n",
        "    try:\n",
        "        # If it's a MultiOutputClassifier, untangle it\n",
        "        if isinstance(clf, MultiOutputClassifier):\n",
        "            # For simplicity, analyze each output separately\n",
        "            for output_idx, estimator in enumerate(clf.estimators_):\n",
        "                logger.info(f\"Analyzing feature importances for output {output_idx + 1} using classifier '{clf_name}'.\")\n",
        "                feature_importances = None\n",
        "\n",
        "                if hasattr(estimator, 'feature_importances_'):\n",
        "                    feature_importances = pd.DataFrame({\n",
        "                        'Feature': feature_names,\n",
        "                        'Importance': estimator.feature_importances_\n",
        "                    })\n",
        "                elif hasattr(estimator, 'coef_'):\n",
        "                    feature_importances = pd.DataFrame({\n",
        "                        'Feature': feature_names,\n",
        "                        'Importance': np.mean(np.abs(estimator.coef_), axis=0)\n",
        "                    })\n",
        "                else:\n",
        "                    # Compute permutation importance\n",
        "                    logger.info(f\"Computing permutation importance for classifier '{clf_name}' on output {output_idx + 1}.\")\n",
        "                    perm = PermutationImportance(estimator, random_state=42, n_iter=10, scoring='f1_macro').fit(X, y[:, output_idx])\n",
        "                    importance_scores = perm.feature_importances_\n",
        "                    feature_importances = pd.DataFrame({\n",
        "                        'Feature': feature_names,\n",
        "                        'Importance': importance_scores\n",
        "                    })\n",
        "\n",
        "                export_feature_importance(\n",
        "                    feature_importances,\n",
        "                    plots_root / f\"{name}_feature_importance_{clf_name}_output_{output_idx + 1}.png\",\n",
        "                    f\"Feature Importance - {clf_name} Output {output_idx + 1}\"\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            feature_importances = None\n",
        "\n",
        "            if hasattr(clf, 'feature_importances_'):\n",
        "                feature_importances = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Importance': clf.feature_importances_\n",
        "                })\n",
        "            elif hasattr(clf, 'coef_'):\n",
        "                feature_importances = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Importance': np.mean(np.abs(clf.coef_), axis=0)\n",
        "                })\n",
        "            else:\n",
        "                # Compute permutation importance\n",
        "                logger.info(f\"Computing permutation importance for classifier '{clf_name}'.\")\n",
        "                perm = PermutationImportance(clf, random_state=42, n_iter=10, scoring='f1_macro').fit(X, y)\n",
        "                importance_scores = perm.feature_importances_\n",
        "                feature_importances = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Importance': importance_scores\n",
        "                })\n",
        "\n",
        "            export_feature_importance(\n",
        "                feature_importances,\n",
        "                plots_root / f\"{name}_feature_importance_{clf_name}.png\",\n",
        "                f\"Feature Importance - {clf_name}\"\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error analyzing feature importances for {clf_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "# -------------------------------\n",
        "# Process Function\n",
        "# -------------------------------\n",
        "\n",
        "def process(name: str, direction: str, features: str, cross_validateq: bool, imbalance_strategy: str, hyperparameter_tuning: bool):\n",
        "    \"\"\"\n",
        "    Processes data and trains models with feature selection, cross-validation, and hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - name (str): Experiment name.\n",
        "    - direction (str): Type of label encoding ('intra' or 'inter').\n",
        "    - features (str): Feature selection method.\n",
        "    - cross_validateq (bool): Whether to perform cross-validation.\n",
        "    - imbalance_strategy (str): Strategy for handling imbalanced data.\n",
        "    - hyperparameter_tuning (bool): Whether to perform hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Starting processing for experiment '{name}' with feature selection '{features}'.\")\n",
        "\n",
        "    # Read and combine data from all flow directories\n",
        "    combined_data = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        for flow_dir in flow_dirs:\n",
        "            device_idx = int(flow_dir.name.replace(\"flow\", \"\"))\n",
        "            device_name = devices_fullname[device_idx - 1].lower()\n",
        "            logger.info(f\"Processing data for {device_name} from {flow_dir}.\")\n",
        "\n",
        "            for app_idx, app in enumerate(apps, start=1):\n",
        "                app_fullname = apps_fullname[app_idx - 1]\n",
        "                filepath = flow_dir / f\"{app_fullname}_filtered_flows.txt\"\n",
        "                logger.info(f\"Reading data from {filepath}.\")\n",
        "                df = read_flow_data(filepath)\n",
        "                if not df.empty:\n",
        "                    df['application'] = app_fullname\n",
        "                    df['device'] = device_name\n",
        "                    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing data for {flow_dir}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if combined_data.empty:\n",
        "        logger.error(\"No data found. Exiting process.\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Total records after combining: {combined_data.shape[0]}.\")\n",
        "\n",
        "    # Choose features based on selection type (handles encoding)\n",
        "    comb_selected = choose_features(combined_data, features, name, typee=direction)\n",
        "\n",
        "    # Encode labels and save combined DataFrame\n",
        "    export_df(comb_selected, csvs / f\"{name}_{features}_out.csv\")\n",
        "    logger.info(f\"Combined DataFrame exported to {csvs / f'{name}_{features}_out.csv'}.\")\n",
        "\n",
        "    # Perform EDA\n",
        "    perform_eda(comb_selected, name)\n",
        "\n",
        "    # Prepare features and labels\n",
        "    X = comb_selected.drop(LABEL_COLUMNS, axis=1)\n",
        "    y_app = LabelEncoder().fit_transform(comb_selected['application'])\n",
        "    y_dev = LabelEncoder().fit_transform(comb_selected['device'])\n",
        "    y_encoded = np.vstack((y_app, y_dev)).T  # Shape: (n_samples, 2)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    logger.info(\"Features standardized using StandardScaler.\")\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, models_dir / f\"{name}_scaler.joblib\")\n",
        "    logger.info(f\"Scaler saved to {models_dir / f'{name}_scaler.joblib'}\")\n",
        "\n",
        "    # Check if y is multi-output\n",
        "    multi_output = y_encoded.ndim > 1 and y_encoded.shape[1] > 1\n",
        "\n",
        "    if multi_output and imbalance_strategy != \"class_weight\":\n",
        "        logger.warning(\n",
        "            f\"Imbalance strategy '{imbalance_strategy}' is not supported for multi-output targets. \"\n",
        "            f\"Setting imbalance_strategy to 'class_weight'.\"\n",
        "        )\n",
        "        imbalance_strategy = \"class_weight\"\n",
        "\n",
        "    # Handle imbalanced data\n",
        "    X_res, y_res = handle_imbalance(X_scaled, y_encoded, strategy=imbalance_strategy)\n",
        "\n",
        "    logger.info(f\"After imbalance handling, X_res shape: {X_res.shape}, y_res shape: {y_res.shape}\")\n",
        "\n",
        "    # Initialize MultiOutputClassifier\n",
        "    # Remove classifiers that do not support class_weight if using 'class_weight'\n",
        "    if imbalance_strategy == \"class_weight\":\n",
        "        adjusted_cdic = {clf_name: clf for clf_name, clf in cdic.items()\n",
        "                         if not (clf_name == \"NaiveBayes\")}  # Exclude GaussianNB\n",
        "    else:\n",
        "        adjusted_cdic = cdic.copy()\n",
        "\n",
        "    classifiers = {clf_name: MultiOutputClassifier(clf) for clf_name, clf in adjusted_cdic.items()}\n",
        "\n",
        "    # Iterate over classifiers\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        logger.info(f\"Processing classifier: {clf_name}\")\n",
        "\n",
        "        # Define the model filename\n",
        "        model_filename = models_dir / f\"{name}_{clf_name}.joblib\"\n",
        "\n",
        "        # Check if the model already exists\n",
        "        if model_filename.exists():\n",
        "            try:\n",
        "                clf_best = joblib.load(model_filename)\n",
        "                logger.info(f\"Loaded existing model for {clf_name} from {model_filename}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading model for {clf_name}: {e}\")\n",
        "                continue\n",
        "        else:\n",
        "            # Model does not exist; proceed to train\n",
        "            logger.info(f\"No existing model found for {clf_name}. Training a new model.\")\n",
        "\n",
        "            # Apply class_weight if the base estimator supports it and strategy is class_weight\n",
        "            if imbalance_strategy == \"class_weight\":\n",
        "                base_estimator = clf.estimator\n",
        "                if hasattr(base_estimator, 'class_weight'):\n",
        "                    base_estimator.class_weight = 'balanced'\n",
        "                    logger.info(f\"Applied class_weight='balanced' to {clf_name}\")\n",
        "\n",
        "            if hyperparameter_tuning:\n",
        "                logger.info(f\"Starting hyperparameter tuning for {clf_name}\")\n",
        "                # Define parameter grid for hyperparameter tuning\n",
        "                param_grid = {}\n",
        "\n",
        "                if \"SVM\" in clf_name:\n",
        "                    param_grid = {\n",
        "                        'estimator__C': [0.1, 1.0, 10.0],\n",
        "                        'estimator__max_iter': [1000, 5000, 10000]\n",
        "                    }\n",
        "                elif \"RandomForest\" in clf_name:\n",
        "                    param_grid = {\n",
        "                        'estimator__n_estimators': [100, 200],\n",
        "                        'estimator__max_depth': [None, 10, 20]\n",
        "                    }\n",
        "                # Add other classifiers' parameter grids as needed\n",
        "\n",
        "                if param_grid:\n",
        "                    # Initialize GridSearchCV with MultiOutputClassifier\n",
        "                    grid_search = GridSearchCV(\n",
        "                        estimator=clf,\n",
        "                        param_grid=param_grid,\n",
        "                        cv=3,\n",
        "                        scoring='f1_macro',\n",
        "                        n_jobs=-1,\n",
        "                        verbose=1\n",
        "                    )\n",
        "                    grid_search.fit(X_res, y_res)\n",
        "                    clf_best = grid_search.best_estimator_\n",
        "                    logger.info(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
        "                else:\n",
        "                    logger.warning(f\"No parameter grid defined for {clf_name}. Skipping hyperparameter tuning.\")\n",
        "                    clf.fit(X_res, y_res)\n",
        "                    clf_best = clf\n",
        "            else:\n",
        "                # Without hyperparameter tuning\n",
        "                logger.info(f\"Training {clf_name} without hyperparameter tuning.\")\n",
        "                clf.fit(X_res, y_res)\n",
        "                clf_best = clf\n",
        "                joblib.dump(clf_best, model_filename)\n",
        "                logger.info(f\"Trained model for {clf_name} saved to {model_filename}\")\n",
        "\n",
        "        # Proceed with evaluation whether the model was loaded or newly trained\n",
        "        logger.info(f\"Evaluating classifier: {clf_name}\")\n",
        "\n",
        "        if cross_validateq:\n",
        "            # 10-fold cross-validation using StratifiedKFold on combined labels\n",
        "            combined_label = y_res[:, 0] * len(devices_fullname) + y_res[:, 1]\n",
        "            cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "            scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "\n",
        "            scores_app = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "            scores_dev = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "            logger.info(f\"Starting cross-validation for {clf_name}...\")\n",
        "\n",
        "            for fold, (train_idx, test_idx) in enumerate(cv.split(X_res, combined_label), 1):\n",
        "                X_train, X_test = X_res[train_idx], X_res[test_idx]\n",
        "                y_train, y_test = y_res[train_idx], y_res[test_idx]\n",
        "\n",
        "                clf_best.fit(X_train, y_train)\n",
        "                y_pred = clf_best.predict(X_test)\n",
        "\n",
        "                # Ensure predictions are numpy arrays\n",
        "                if isinstance(y_pred, pd.Series) or isinstance(y_pred, pd.DataFrame):\n",
        "                    y_pred = y_pred.to_numpy()\n",
        "                y_pred = y_pred.astype(int)\n",
        "\n",
        "                # Calculate metrics for 'application'\n",
        "                scores_app['accuracy'].append(accuracy_score(y_test[:, 0], y_pred[:, 0]))\n",
        "                scores_app['precision'].append(precision_score(y_test[:, 0], y_pred[:, 0], average='macro', zero_division=0))\n",
        "                scores_app['recall'].append(recall_score(y_test[:, 0], y_pred[:, 0], average='macro', zero_division=0))\n",
        "                scores_app['f1'].append(sk_f1_score(y_test[:, 0], y_pred[:, 0], average='macro', zero_division=0))\n",
        "\n",
        "                # Calculate metrics for 'device'\n",
        "                scores_dev['accuracy'].append(accuracy_score(y_test[:, 1], y_pred[:, 1]))\n",
        "                scores_dev['precision'].append(precision_score(y_test[:, 1], y_pred[:, 1], average='macro', zero_division=0))\n",
        "                scores_dev['recall'].append(recall_score(y_test[:, 1], y_pred[:, 1], average='macro', zero_division=0))\n",
        "                scores_dev['f1'].append(sk_f1_score(y_test[:, 1], y_pred[:, 1], average='macro', zero_division=0))\n",
        "\n",
        "            # Export fold scores\n",
        "            df_scores_app = pd.DataFrame(scores_app)\n",
        "            df_scores_dev = pd.DataFrame(scores_dev)\n",
        "            export_df(df_scores_app, csvs / f\"{name}_fold_{clf_name}_application.csv\")\n",
        "            export_df(df_scores_dev, csvs / f\"{name}_fold_{clf_name}_device.csv\")\n",
        "            logger.info(f\"Cross-validation scores for {clf_name} exported.\\n\")\n",
        "\n",
        "            # Fit on the entire dataset for confusion matrix and feature importance\n",
        "            clf_best.fit(X_res, y_res)\n",
        "            y_predic = clf_best.predict(X_res)\n",
        "\n",
        "            # Ensure predictions are numpy arrays\n",
        "            if isinstance(y_predic, pd.Series) or isinstance(y_predic, pd.DataFrame):\n",
        "                y_predic = y_predic.to_numpy()\n",
        "            y_predic = y_predic.astype(int)\n",
        "\n",
        "            # Generate confusion matrices\n",
        "            cm_app = confusion_matrix(y_res[:, 0], y_predic[:, 0], labels=range(len(apps_fullname)))\n",
        "            cm_dev = confusion_matrix(y_res[:, 1], y_predic[:, 1], labels=range(len(devices_fullname)))\n",
        "\n",
        "            export_cm(cm_app, apps_fullname, plots_root / f\"{name}_cm_{clf_name}_application.png\", \"Confusion Matrix - Application\")\n",
        "            export_cm(cm_dev, devices_fullname, plots_root / f\"{name}_cm_{clf_name}_device.png\", \"Confusion Matrix - Device\")\n",
        "\n",
        "            # Feature Importance Analysis\n",
        "            analyze_feature_importance(clf_best, X_res, y_res, X.columns.tolist(), name, clf_name)\n",
        "\n",
        "            # Generate classification reports\n",
        "            report_app = classification_report(y_res[:, 0], y_predic[:, 0], target_names=apps_fullname, zero_division=0)\n",
        "            report_dev = classification_report(y_res[:, 1], y_predic[:, 1], target_names=devices_fullname, zero_division=0)\n",
        "\n",
        "            # Compute overall accuracy\n",
        "            accuracy_app = accuracy_score(y_res[:, 0], y_predic[:, 0])\n",
        "            accuracy_dev = accuracy_score(y_res[:, 1], y_predic[:, 1])\n",
        "\n",
        "            # Append accuracy to the classification reports\n",
        "            report_app += f\"\\nAccuracy: {accuracy_app:.4f}\\n\"\n",
        "            report_dev += f\"\\nAccuracy: {accuracy_dev:.4f}\\n\"\n",
        "\n",
        "            with open(csvs / f\"{name}_report_{clf_name}_application.txt\", \"w\") as f:\n",
        "                f.write(f\"Classification Report for Application - {clf_name}\\n\")\n",
        "                f.write(report_app)\n",
        "\n",
        "            with open(csvs / f\"{name}_report_{clf_name}_device.txt\", \"w\") as f:\n",
        "                f.write(f\"Classification Report for Device - {clf_name}\\n\")\n",
        "                f.write(report_dev)\n",
        "\n",
        "            logger.info(f\"Classification reports for {clf_name} saved.\\n\")\n",
        "\n",
        "        else:\n",
        "            # Without cross-validation: Train on all data and evaluate on the same data (not recommended)\n",
        "            logger.info(f\"Training and evaluating {clf_name} without cross-validation.\")\n",
        "            clf_best.fit(X_res, y_res)\n",
        "            y_pred = clf_best.predict(X_res)\n",
        "\n",
        "            # Ensure predictions are numpy arrays\n",
        "            if isinstance(y_pred, pd.Series) or isinstance(y_pred, pd.DataFrame):\n",
        "                y_pred = y_pred.to_numpy()\n",
        "            y_pred = y_pred.astype(int)\n",
        "\n",
        "            # Classification reports\n",
        "            report_app = classification_report(y_res[:, 0], y_pred[:, 0], target_names=apps_fullname, zero_division=0)\n",
        "            report_dev = classification_report(y_res[:, 1], y_pred[:, 1], target_names=devices_fullname, zero_division=0)\n",
        "\n",
        "            # Compute overall accuracy\n",
        "            accuracy_app = accuracy_score(y_res[:, 0], y_pred[:, 0])\n",
        "            accuracy_dev = accuracy_score(y_res[:, 1], y_pred[:, 1])\n",
        "\n",
        "            # Append accuracy to the classification reports\n",
        "            report_app += f\"\\nAccuracy: {accuracy_app:.4f}\\n\"\n",
        "            report_dev += f\"\\nAccuracy: {accuracy_dev:.4f}\\n\"\n",
        "\n",
        "            with open(csvs / f\"{name}_report_{clf_name}_application.txt\", \"w\") as f:\n",
        "                f.write(f\"Classification Report for Application - {clf_name}\\n\")\n",
        "                f.write(report_app)\n",
        "\n",
        "            with open(csvs / f\"{name}_report_{clf_name}_device.txt\", \"w\") as f:\n",
        "                f.write(f\"Classification Report for Device - {clf_name}\\n\")\n",
        "                f.write(report_dev)\n",
        "\n",
        "            logger.info(f\"Classification reports for {clf_name} saved.\\n\")\n",
        "\n",
        "            # Confusion matrices\n",
        "            cm_app = confusion_matrix(y_res[:, 0], y_pred[:, 0], labels=range(len(apps_fullname)))\n",
        "            cm_dev = confusion_matrix(y_res[:, 1], y_pred[:, 1], labels=range(len(devices_fullname)))\n",
        "\n",
        "            export_cm(cm_app, apps_fullname, plots_root / f\"{name}_cm_{clf_name}_application.png\", \"Confusion Matrix - Application\")\n",
        "            export_cm(cm_dev, devices_fullname, plots_root / f\"{name}_cm_{clf_name}_device.png\", \"Confusion Matrix - Device\")\n",
        "\n",
        "            # Feature Importance Analysis\n",
        "            analyze_feature_importance(clf_best, X_res, y_res, X.columns.tolist(), name, clf_name)\n",
        "\n",
        "# -------------------------------\n",
        "# Main Function\n",
        "# -------------------------------\n",
        "\n",
        "def main(experiment_name=\"exp1\", function=\"process\"):\n",
        "    \"\"\"\n",
        "    Main function to execute the specified function.\n",
        "\n",
        "    Parameters:\n",
        "    - experiment_name (str): Name of the experiment.\n",
        "    - function (str): Function to execute ('process', 'count', 'eda', 'train_model').\n",
        "    \"\"\"\n",
        "\n",
        "    if function == \"process\":\n",
        "        # Example parameters; these should be replaced or extended as needed\n",
        "        feature_options = [\"mutual_info\", \"all\", \"categorical\", \"statistical\", \"custom_statistical\", \"anova_f_test\"]\n",
        "\n",
        "        for feature in feature_options:\n",
        "            process(\n",
        "                name=experiment_name,\n",
        "                direction=\"intra\",  # Options: \"intra\" or \"inter\"\n",
        "                features=feature,  # Use the loop variable 'feature'\n",
        "                cross_validateq=True,  # Set to True for cross-validation\n",
        "                imbalance_strategy=\"smoteenn\",  # Options: \"smote\", \"undersample\", \"class_weight\", \"smoteenn\"\n",
        "                hyperparameter_tuning=True  # Set to True to perform hyperparameter tuning\n",
        "            )\n",
        "    elif function == \"count\":\n",
        "        try:\n",
        "            # Update the filename based on the feature selection used during 'process'\n",
        "            combined_csv = csvs / f\"{experiment_name}_mutual_info_out.csv\"\n",
        "            if not combined_csv.exists():\n",
        "                logger.error(\"Combined CSV not found. Please run the 'process' function first.\")\n",
        "                sys.exit(1)\n",
        "            comb = pd.read_csv(combined_csv)\n",
        "            build_count_table(comb, experiment_name)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\"Combined CSV not found. Please run the 'process' function first.\")\n",
        "    elif function == \"eda\":\n",
        "        try:\n",
        "            # Update the filename based on the feature selection used during 'process'\n",
        "            combined_csv = csvs / f\"{experiment_name}_mutual_info_out.csv\"\n",
        "            if not combined_csv.exists():\n",
        "                logger.error(\"Combined CSV not found. Please run the 'process' function first.\")\n",
        "                sys.exit(1)\n",
        "            comb = pd.read_csv(combined_csv)\n",
        "            perform_eda(comb, experiment_name)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\"Combined CSV not found. Please run the 'process' function first.\")\n",
        "    elif function == \"train_model\":\n",
        "        logger.info(\"Function 'train_model' is reserved for future implementations.\")\n",
        "        # Placeholder for model training and saving functionalities\n",
        "    else:\n",
        "        logger.error(f\"Unknown function: {function}. Available functions: process, count, eda, train_model\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Default parameters for Google Colab\n",
        "    experiment_name = \"last-one\"\n",
        "    function = \"process\"  # Options: \"process\", \"count\", \"eda\", \"train_model\"\n",
        "\n",
        "    # Run the main function with default parameters\n",
        "    main(experiment_name, function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "Aw-_Bi8bfskL",
        "outputId": "b5dc04e6-8d8e-476a-cf34-6093c34bd70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Imbalance strategy 'smoteenn' is not supported for multi-output targets. Setting imbalance_strategy to 'class_weight'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /__w/cuml/cuml/python/build/cp310-cp310-manylinux_2_17_x86_64/_deps/rmm-src/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-af417d59b8d3>\u001b[0m in \u001b[0;36m<cell line: 956>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[0;31m# Run the main function with default parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-af417d59b8d3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(experiment_name, function)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             process(\n\u001b[0m\u001b[1;32m    920\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                 \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"intra\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Options: \"intra\" or \"inter\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-af417d59b8d3>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(name, direction, features, cross_validateq, imbalance_strategy, hyperparameter_tuning)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mclf_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \"\"\"\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mfit_params_validated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    217\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    218\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_validated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC._fit_proba\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC._fit_proba\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/calibration.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 self.calibrated_classifiers_ = parallel(\n\u001b[0m\u001b[1;32m    396\u001b[0m                     delayed(_fit_classifier_calibrator_pair)(\n\u001b[1;32m    397\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/calibration.py\u001b[0m in \u001b[0;36m_fit_classifier_calibrator_pair\u001b[0;34m(estimator, X, y, train, test, supports_sw, method, classes, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msw_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC._fit_multiclass\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/multiclass/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_to_host_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_internal_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    688\u001b[0m             zip(\n\u001b[1;32m    689\u001b[0m                 *(\n\u001b[0;32m--> 690\u001b[0;31m                     Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    691\u001b[0m                         delayed(_fit_ovo_binary)(\n\u001b[1;32m    692\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_ovo_binary\u001b[0;34m(estimator, X, y, i, j)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0mindcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     return (\n\u001b[0;32m--> 549\u001b[0;31m         _fit_binary(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msvc.pyx\u001b[0m in \u001b[0;36mcuml.svm.svc.SVC.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /__w/cuml/cuml/python/build/cp310-cp310-manylinux_2_17_x86_64/_deps/rmm-src/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory"
          ]
        }
      ]
    }
  ]
}